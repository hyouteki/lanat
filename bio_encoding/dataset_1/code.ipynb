{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load JSON data from file\n",
    "file_path = r'C:\\Users\\HP\\OneDrive\\Desktop\\a2_nlp\\NER_TRAIN_JUDGEMENT.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extracting text and annotations\n",
    "dataset = {}\n",
    "for entry in data:\n",
    "    case_id = entry['id']\n",
    "    text = entry['data']['text']\n",
    "    annotations = entry['annotations'][0]['result']\n",
    "\n",
    "    # Create a list of tuples with start, end, and label for each entity\n",
    "    entities = [(entity['value']['start'], entity['value']['end'], entity['value']['labels'][0])\n",
    "                for entity in annotations]\n",
    "    \n",
    "    # Initialize labels list with 'O' for each token in the text\n",
    "    labels = ['O'] * len(text.split())\n",
    "    # print(labels)\n",
    "    print(entities)\n",
    "    # Assign BIO labels to tokens based on entity spans\n",
    "    print(text.split())\n",
    "    for start, end, label in entities:\n",
    "        start_idx = len(text[:start].split())\n",
    "        end_idx = len(text[:end].split())\n",
    "        print(start_idx,end_idx)\n",
    "        if start_idx == end_idx:\n",
    "            labels[start_idx] = 'B_' + label\n",
    "        else:\n",
    "            labels[start_idx] = 'B_' + label\n",
    "            # print(labels[start_idx + 1:end_idx+1 ])\n",
    "            labels[start_idx + 1:end_idx+1 ] = ['I_' + label] * (end_idx - start_idx)\n",
    "\n",
    "\n",
    "    # Store the result in the dataset dictionary\n",
    "    dataset[case_id] = {'text': text, 'labels': labels}\n",
    "\n",
    "    # break\n",
    "\n",
    "# Printing the dataset dictionary with id, text, and labels\n",
    "# Printing the dataset with span information\n",
    "for case_id, values in dataset.items():\n",
    "    print(f\"ID: {case_id}\")\n",
    "    print(f\"Text: {values['text']}\")\n",
    "    print(\"Labels:\", values['labels'])\n",
    "    \n",
    "    # Print start and end positions for each token\n",
    "    for token, label in zip(values['text'].split(), values['labels']):\n",
    "        start = values['text'].find(token)\n",
    "        end = start + len(token)\n",
    "        # print(f\"Token: {token}, Span: ({start}, {end}), Label: {label}\")\n",
    "    \n",
    "    print('-' * 50)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "# train_dataset, val_dataset = train_test_split(list(dataset.items()), test_size=0.15, random_state=42)\n",
    "\n",
    "# # Convert the split datasets to dictionaries\n",
    "# train_dataset = dict(train_dataset)\n",
    "# val_dataset = dict(val_dataset)\n",
    "\n",
    "# # Save the datasets to separate JSON files\n",
    "# with open('train_dataset.json', 'w', encoding='utf-8') as train_file:\n",
    "#     json.dump(train_dataset, train_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# with open('val_dataset.json', 'w', encoding='utf-8') as val_file:\n",
    "#     json.dump(val_dataset, val_file, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
