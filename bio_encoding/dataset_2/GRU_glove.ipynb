{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Validation Loss: 0.1888\n",
      "Validation Accuracy: 0.9666, Validation Macro F1: 0.3277\n",
      "Epoch 2/60, Validation Loss: 0.1538\n",
      "Validation Accuracy: 0.9670, Validation Macro F1: 0.3410\n",
      "Epoch 3/60, Validation Loss: 0.1344\n",
      "Validation Accuracy: 0.9683, Validation Macro F1: 0.3769\n",
      "Epoch 4/60, Validation Loss: 0.1196\n",
      "Validation Accuracy: 0.9687, Validation Macro F1: 0.4225\n",
      "Epoch 5/60, Validation Loss: 0.1101\n",
      "Validation Accuracy: 0.9697, Validation Macro F1: 0.4396\n",
      "Epoch 6/60, Validation Loss: 0.1025\n",
      "Validation Accuracy: 0.9722, Validation Macro F1: 0.4735\n",
      "Epoch 7/60, Validation Loss: 0.0958\n",
      "Validation Accuracy: 0.9744, Validation Macro F1: 0.5413\n",
      "Epoch 8/60, Validation Loss: 0.0908\n",
      "Validation Accuracy: 0.9745, Validation Macro F1: 0.5698\n",
      "Epoch 9/60, Validation Loss: 0.0866\n",
      "Validation Accuracy: 0.9755, Validation Macro F1: 0.6249\n",
      "Epoch 10/60, Validation Loss: 0.0837\n",
      "Validation Accuracy: 0.9759, Validation Macro F1: 0.6451\n",
      "Epoch 11/60, Validation Loss: 0.0810\n",
      "Validation Accuracy: 0.9769, Validation Macro F1: 0.6814\n",
      "Epoch 12/60, Validation Loss: 0.0786\n",
      "Validation Accuracy: 0.9768, Validation Macro F1: 0.6836\n",
      "Epoch 13/60, Validation Loss: 0.0769\n",
      "Validation Accuracy: 0.9770, Validation Macro F1: 0.6784\n",
      "Epoch 14/60, Validation Loss: 0.0754\n",
      "Validation Accuracy: 0.9770, Validation Macro F1: 0.6785\n",
      "Epoch 15/60, Validation Loss: 0.0733\n",
      "Validation Accuracy: 0.9780, Validation Macro F1: 0.7065\n",
      "Epoch 16/60, Validation Loss: 0.0724\n",
      "Validation Accuracy: 0.9784, Validation Macro F1: 0.7047\n",
      "Epoch 17/60, Validation Loss: 0.0709\n",
      "Validation Accuracy: 0.9781, Validation Macro F1: 0.7202\n",
      "Epoch 18/60, Validation Loss: 0.0692\n",
      "Validation Accuracy: 0.9786, Validation Macro F1: 0.7264\n",
      "Epoch 19/60, Validation Loss: 0.0686\n",
      "Validation Accuracy: 0.9789, Validation Macro F1: 0.7221\n",
      "Epoch 20/60, Validation Loss: 0.0672\n",
      "Validation Accuracy: 0.9789, Validation Macro F1: 0.7204\n",
      "Epoch 21/60, Validation Loss: 0.0673\n",
      "Validation Accuracy: 0.9793, Validation Macro F1: 0.7219\n",
      "Epoch 22/60, Validation Loss: 0.0664\n",
      "Validation Accuracy: 0.9791, Validation Macro F1: 0.7341\n",
      "Epoch 23/60, Validation Loss: 0.0652\n",
      "Validation Accuracy: 0.9800, Validation Macro F1: 0.7492\n",
      "Epoch 24/60, Validation Loss: 0.0660\n",
      "Validation Accuracy: 0.9791, Validation Macro F1: 0.7521\n",
      "Epoch 25/60, Validation Loss: 0.0642\n",
      "Validation Accuracy: 0.9799, Validation Macro F1: 0.7464\n",
      "Epoch 26/60, Validation Loss: 0.0640\n",
      "Validation Accuracy: 0.9802, Validation Macro F1: 0.7598\n",
      "Epoch 27/60, Validation Loss: 0.0643\n",
      "Validation Accuracy: 0.9803, Validation Macro F1: 0.7472\n",
      "Epoch 28/60, Validation Loss: 0.0653\n",
      "Validation Accuracy: 0.9799, Validation Macro F1: 0.7363\n",
      "Epoch 29/60, Validation Loss: 0.0648\n",
      "Validation Accuracy: 0.9783, Validation Macro F1: 0.7537\n",
      "Epoch 30/60, Validation Loss: 0.0652\n",
      "Validation Accuracy: 0.9799, Validation Macro F1: 0.7426\n",
      "Epoch 31/60, Validation Loss: 0.0658\n",
      "Validation Accuracy: 0.9803, Validation Macro F1: 0.7440\n",
      "Epoch 32/60, Validation Loss: 0.0657\n",
      "Validation Accuracy: 0.9800, Validation Macro F1: 0.7341\n",
      "Epoch 33/60, Validation Loss: 0.0643\n",
      "Validation Accuracy: 0.9792, Validation Macro F1: 0.7575\n",
      "Epoch 34/60, Validation Loss: 0.0643\n",
      "Validation Accuracy: 0.9797, Validation Macro F1: 0.7577\n",
      "Epoch 35/60, Validation Loss: 0.0675\n",
      "Validation Accuracy: 0.9801, Validation Macro F1: 0.7399\n",
      "Epoch 36/60, Validation Loss: 0.0670\n",
      "Validation Accuracy: 0.9800, Validation Macro F1: 0.7590\n",
      "Epoch 37/60, Validation Loss: 0.0660\n",
      "Validation Accuracy: 0.9791, Validation Macro F1: 0.7536\n",
      "Epoch 38/60, Validation Loss: 0.0662\n",
      "Validation Accuracy: 0.9786, Validation Macro F1: 0.7562\n",
      "Epoch 39/60, Validation Loss: 0.0661\n",
      "Validation Accuracy: 0.9791, Validation Macro F1: 0.7568\n",
      "Epoch 40/60, Validation Loss: 0.0684\n",
      "Validation Accuracy: 0.9799, Validation Macro F1: 0.7439\n",
      "Epoch 41/60, Validation Loss: 0.0680\n",
      "Validation Accuracy: 0.9794, Validation Macro F1: 0.7562\n",
      "Epoch 42/60, Validation Loss: 0.0685\n",
      "Validation Accuracy: 0.9788, Validation Macro F1: 0.7487\n",
      "Epoch 43/60, Validation Loss: 0.0727\n",
      "Validation Accuracy: 0.9788, Validation Macro F1: 0.7335\n",
      "Epoch 44/60, Validation Loss: 0.0761\n",
      "Validation Accuracy: 0.9790, Validation Macro F1: 0.7292\n",
      "Epoch 45/60, Validation Loss: 0.0739\n",
      "Validation Accuracy: 0.9778, Validation Macro F1: 0.7381\n",
      "Epoch 46/60, Validation Loss: 0.0776\n",
      "Validation Accuracy: 0.9796, Validation Macro F1: 0.7420\n",
      "Epoch 47/60, Validation Loss: 0.0758\n",
      "Validation Accuracy: 0.9777, Validation Macro F1: 0.7358\n",
      "Epoch 48/60, Validation Loss: 0.0764\n",
      "Validation Accuracy: 0.9781, Validation Macro F1: 0.7392\n",
      "Epoch 49/60, Validation Loss: 0.0778\n",
      "Validation Accuracy: 0.9780, Validation Macro F1: 0.7386\n",
      "Epoch 50/60, Validation Loss: 0.0807\n",
      "Validation Accuracy: 0.9780, Validation Macro F1: 0.7402\n",
      "Epoch 51/60, Validation Loss: 0.0823\n",
      "Validation Accuracy: 0.9779, Validation Macro F1: 0.7378\n",
      "Epoch 52/60, Validation Loss: 0.0842\n",
      "Validation Accuracy: 0.9779, Validation Macro F1: 0.7379\n",
      "Epoch 53/60, Validation Loss: 0.0882\n",
      "Validation Accuracy: 0.9788, Validation Macro F1: 0.7390\n",
      "Epoch 54/60, Validation Loss: 0.0845\n",
      "Validation Accuracy: 0.9781, Validation Macro F1: 0.7262\n",
      "Epoch 55/60, Validation Loss: 0.0866\n",
      "Validation Accuracy: 0.9780, Validation Macro F1: 0.7349\n",
      "Epoch 56/60, Validation Loss: 0.0855\n",
      "Validation Accuracy: 0.9775, Validation Macro F1: 0.7318\n",
      "Epoch 57/60, Validation Loss: 0.0857\n",
      "Validation Accuracy: 0.9768, Validation Macro F1: 0.7381\n",
      "Epoch 58/60, Validation Loss: 0.0877\n",
      "Validation Accuracy: 0.9769, Validation Macro F1: 0.7365\n",
      "Epoch 59/60, Validation Loss: 0.0890\n",
      "Validation Accuracy: 0.9769, Validation Macro F1: 0.7381\n",
      "Epoch 60/60, Validation Loss: 0.0929\n",
      "Validation Accuracy: 0.9769, Validation Macro F1: 0.7305\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(file_path, word_to_index, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    num_words = len(word_to_index) + 1  # Add 1 for the padding token\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, index in word_to_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[index] = embeddings_index.get(\"<unk>\", np.zeros(embedding_dim))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "with open(\"processed_train_data.json\", \"r\") as f_train, open(\"processed_val_data.json\", \"r\") as f_val:\n",
    "    processed_train_data = json.load(f_train)\n",
    "    processed_val_data = json.load(f_val)\n",
    "\n",
    "\n",
    "texts_train = [entry[\"text\"] for entry in processed_train_data.values()]\n",
    "labels_train = [entry[\"labels\"] for entry in processed_train_data.values()]\n",
    "\n",
    "texts_val = [entry[\"text\"] for entry in processed_val_data.values()]\n",
    "labels_val = [entry[\"labels\"] for entry in processed_val_data.values()]\n",
    "\n",
    "\n",
    "word_to_index = {}\n",
    "index = 1 \n",
    "for entry in processed_train_data.values():\n",
    "    tokens = entry[\"text\"].split() \n",
    "    for token in tokens:\n",
    "        if token not in word_to_index:\n",
    "            word_to_index[token] = index\n",
    "            index += 1\n",
    "\n",
    "word_to_index[\"<unk>\"] = 0\n",
    "embedding_dim = 100\n",
    "\n",
    "glove_embeddings_path = \"glove.6B.100d.txt\"  \n",
    "embedding_matrix = load_glove_embeddings(glove_embeddings_path, word_to_index, embedding_dim)\n",
    "\n",
    "label_to_index = {'O': 0, 'B': 1, 'I': 2}\n",
    "\n",
    "X_train = [[word_to_index.get(token, word_to_index[\"<unk>\"]) for token in text.split()] for text in texts_train]\n",
    "y_train = [[label_to_index[label] for label in entry] for entry in labels_train] \n",
    "X_val = [[word_to_index.get(token, word_to_index[\"<unk>\"]) for token in text.split()] for text in texts_val]\n",
    "y_val = [[label_to_index[label] for label in entry] for entry in labels_val] \n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.X[idx]), torch.LongTensor(self.y[idx])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, output_size):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0)\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "hidden_size = 100\n",
    "output_size = len(label_to_index)\n",
    "model = SimpleGRU(embedding_matrix, hidden_size, output_size)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, output_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, output_size), targets.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    " \n",
    "            preds = torch.argmax(outputs, dim=2).cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    macro_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    print(f'Validation Accuracy: {accuracy:.4f}, Validation Macro F1: {macro_f1:.4f}')\n",
    "torch.save(model.state_dict(),\"models/gru_glove.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9655, Final Test Macro F1: 0.6604\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load processed test data from JSON\n",
    "with open(\"processed_test_data.json\", \"r\") as f_test:\n",
    "    processed_test_data = json.load(f_test)\n",
    "\n",
    "# Extract text and labels from the dictionary\n",
    "texts_test = [entry[\"text\"] for entry in processed_test_data.values()]\n",
    "labels_test = [entry[\"labels\"] for entry in processed_test_data.values()]\n",
    "\n",
    "# Convert texts and labels to numerical format\n",
    "X_test = [[word_to_index.get(token, word_to_index[\"<unk>\"]) for token in text.split()] for text in texts_test]\n",
    "y_test = [[label_to_index[label] for label in entry] for entry in labels_test]\n",
    "\n",
    "# Create a custom dataset for the test data\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Run the model on the test data after 60 epochs\n",
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, output_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model.eval()\n",
    "all_preds_test = []\n",
    "all_targets_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs_test, targets_test in test_loader:\n",
    "        outputs_test = model(inputs_test)\n",
    "        preds_test = torch.argmax(outputs_test, dim=2).cpu().numpy()\n",
    "        targets_test = targets_test.cpu().numpy()\n",
    "\n",
    "        all_preds_test.extend(preds_test)\n",
    "        all_targets_test.extend(targets_test)\n",
    "\n",
    "# Flatten predictions and targets for evaluation\n",
    "all_preds_test = np.concatenate(all_preds_test, axis=0)\n",
    "all_targets_test = np.concatenate(all_targets_test, axis=0)\n",
    "\n",
    "# Calculate accuracy and macro F1 score on test data\n",
    "test_accuracy = accuracy_score(all_targets_test, all_preds_test)\n",
    "test_macro_f1 = f1_score(all_targets_test, all_preds_test, average='macro')\n",
    "\n",
    "print(f'Final Test Accuracy: {test_accuracy:.4f}, Final Test Macro F1: {test_macro_f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
